# 开放端口
# <source>
#   @type forward
#   port 24224
#   bind 0.0.0.0
# </source>

<source>
  @type tail                      # 内置的输入方式，从源文件中获取新的日志。
  path /usr/local/logs/*.log         # 挂载的服务器 Docker 容器日志地址
  pos_file /usr/local/logs/*.log.pos
  tag test.*                     # 设置日志标签
  read_from_head true
  <parse>
    @type multiline
    format_firstline /\d{4}-\d{1,2}-\d{1,2}/  #匹配日期开头
    format1 /^(?<time>\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}\.\d{3}) \[(?<thread>[^\]]+)\] (?<level>[^\s]+) (?<message>.*)/
  </parse>
</source>

#排除info以下的日志
# <filter srh.log>
#     @type grep
#     <exclude>
#         key level
#         pattern ^INFO$
#     </exclude>
# </filter>

#添加字段
# <filter srh.log>
#     @type record_transformer
#     <record>
#         hostname "#{Socket.gethostname}"
#         tag ${tag}
#     </record>
# </filter>


<match **>
  @id elasticsearch           # 唯一标识符
  @type elasticsearch         # elasticsearch 插件
#   @log_level info
  host "192.168.0.110"         #需要配置你IP
  port "9200"
  user "elastic"
  password "B6P0hW7x"
  logstash_format true
  logstash_prefix test
  logstash_dateformat %Y-%m-%d
  include_tag_key true
  tag_key @log_name
  <buffer>
    @type file                # 使用文件将缓冲区块存储在磁盘上
    path /usr/local/logs/fluentd.buffer
    flush_mode interval
    retry_type exponential_backoff
    flush_thread_count 2
    flush_interval 5s
    retry_forever
    retry_max_interval 30
    overflow_action block
  </buffer>
</match>

# <match **>
#   @type copy
#   <store>
#     @type elasticsearch
#     host 192.168.10.109
#     port 9200
#     user elastic
#     password B6P0hW7x
#     logstash_format true
#     logstash_prefix srh2
#     logstash_dateformat %Y-%m-%d
#     include_tag_key true
#     tag_key @log_name
#     flush_interval 1s
#   </store>
#   <store>
#     @type stdout
#   </store>
# </match>
